[{"title":"对于卷积的理解","url":"/2022/12/04/对于卷积的理解/","content":"\n\n\n# 对于卷积的理解\n<!--more-->\n#### 为什么卷积参数，相比全连接这么少却可以奏效\n\n一句话：利用了其图像有效特征的局部性，平移不变性。下面我会举例子的方式对其具体的过程进行剖析：\n\n#### 对于全连接\n\nw*x（上一层的几千个）=x（下一层的一个节点），有多少个w就有多少个下一层节点，最后希望汇总得到的节点无限逼近于 0 0 1。以此反向传播不断调节参数。这个我们很容易理解，相当于一个方程，等式右边的结果给你了，未知数x的值也都给你了，就一直通过梯度反向传播求每一个x的参数w。这很容易理解。*\n\n#### 对于卷积\n\n*w（不断复用的几个）* x（含有空间信息的几千个）=x（下一层）,最后我们依然希望可以汇总得到独热编码 0 0 1。仍然可以套用方程式的理解，给定等式右边，以及x，求w。\n\n但是我们不难看出，w只有几个，怎么做才能那么精巧的训练出刚好使得等式右边为 0 0 1呢。我们不难想象，在训练过程中，w与含有空间信息的x进行卷积操作（即交叉相乘），我们最终一定可以训练出一个w。对于，对图像类别起决定性作用的图像的像素位置区域，进行交叉相乘时，会更加敏感，能将其数值保留下来，保留到下一层。影响最后的结果 0 0 1。对于无关信息的图像像素，进行交叉相乘后，留给下一层的东西，很小，很少，几乎不会影响最后的结果 0 0 1。这也就解释了，所谓特征提取，特征融合，到底是在做什么。为什么卷积，可以奏效。\n\n将卷积神经网络的训练进行可视化，也可以证实我的上述想法。\n\n#### 对于通道\n\n每一个通道，会学到不完全一样的特征。在（64通道）往下一层（128通道，64通道，10通道等等任意通道）的时候，又必然会进行一次64通道的特征融合，加上不同方式（128种，64种，10种等任意种）的特征提取。（每一个通道都是一种特征提取方式）\n\n于是，我们每一层都在做特征提取，特征融合，期望我们通过训练卷积核（即w）提取到的特征x，能够影响到最后的 0 0 1的生成。如果不利于，则会调整参数改变特征提取方式，利于，也会调整参数改善特征提取方式。\n","tags":["卷积"],"categories":["deep learning"]},{"title":"矩池云解压方式","url":"/2022/11/26/矩池云解压方式/","content":"\n# 矩池云解压方式\n<!--more-->\n## 第一步 进入机器\n\n可以现在ssh，我这里选择的是jupyter。\n\n进入命令行\n\n## 第二步 进入相应目录 并解压\n\n![](https://pica.zhimg.com/50/v2-17487ca09d5c527551caa8ea41d751e5_720w.jpg?source=1940ef5c)\n\n![](https://pica.zhimg.com/80/v2-17487ca09d5c527551caa8ea41d751e5_1440w.webp?source=1940ef5c)\n\n![](https://pic1.zhimg.com/50/v2-c9ce7090fce5c0e92dde2144d212b838_720w.jpg?source=1940ef5c)\n\n![](https://pic1.zhimg.com/80/v2-c9ce7090fce5c0e92dde2144d212b838_1440w.webp?source=1940ef5c)\n\n我这里以zip压缩包为例，因为Ubuntu自带了unzip所以直接使用就好。\n\n## 第三步 其他压缩包\n\n如果是RAR/tar.gz/gz这些压缩包的用户，命令如下\n\n**gz解压**\n\n```text\ngzip -d file.gz\n```\n\n**tar.gz解压**\n\n```text\ntar -xzvf myetc.tar.gz \n```\n\n**rar解压**\n\n```text\napt-get update\napt-get install rar unrar\nunrar  x  test.rar\n```\n\n  因为矩池云里面是一个Ubuntu系统，基本上Ubuntu的命令都是适用的。\n","tags":["解压"],"categories":["钜池云教程"]},{"title":"权重的保存与加载","url":"/2022/11/20/权重的保存与加载/","content":"\n\n# 权重的保存与加载\n<!--more-->\n**(1) 保存和加载整个模型**\n\n```ruby\n# 模型保存\ntorch.save(model, 'model.pkl')\n# 模型加载\nmodel = torch.load('model.pkl')\n```\n\n这种方式无需自定义网络，保存时已把网络结构保存，比较死板，不能调整网络结构。\n\n**(2) 仅仅保存模型参数以及分别加载模型结构和参数**\n\n```ruby\n# 模型参数保存\ntorch.save(model.state_dict(), 'model_param.pkl')\n# 模型参数加载\nmodel = ModelClass(...)\nmodel.load_state_dict(torch.load('model_param.pkl'))\n```\n\n这种方式需要自己定义网络，并且其中的参数名称与结构要与保存的模型中的一致（可以是部分网络，比如只使用VGG的前几层），相对灵活，便于对网络进行修改。\n\n说明：\n\n1.[torch](https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020).load加载模型参数\n\n2.[model](https://so.csdn.net/so/search?q=model&spm=1001.2101.3001.7020).load\\_state\\_dict加载模型参数到模型结构\n\n## CPU模型加载GPU参数\n\n```lua\nmodel.load_state_dict(torch.load('model_param.pkl', map_location='cpu'))\n```\n\n## 通过DataParalle使用多GPU\n\n```ruby\nmodel=DataParalle(model)\n#保存参数\ntorch.save(model.module.state_dict(), 'model_param.pkl')\n```\n\n**(3) `**pytorch加载预训练模型**`****\n\n## **`加载预训练模型和参数**`\n\n```sql\nresnet18 = models.resnet(pretrained=True)\n```\n\n## `只加载模型，不加载预训练参数`\n\n```python\n# 加载模型\nresnet18 = models.resnet18(pretrained=False)\n# 加载预先下载好的预训练模型参数\nresnet18.load_state_dict(torch.load('resnet18-5c106cde.pth'))\n```\n\n## `加载部分预训练模型`\n\n```python\nresnet152 = models.resnet152(pretrained=True)\npretrained_dict = resnet152.state_dict()\n\"\"\"加载torchvision中的预训练模型和参数后通过state_dict()方法提取参数   也可以直接从官方model_zoo下载：   pretrained_dict = model_zoo.load_url(model_urls['resnet152'])\"\"\"\nmodel_dict = model.state_dict()\n# 将pretrained_dict里不属于model_dict的键剔除掉\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n# 更新现有的model_dict\nmodel_dict.update(pretrained_dict)\n# 加载我们真正需要的state_dict\nmodel.load_state_dict(model_dict)\n```\n","tags":["权重保存"],"categories":["deep learning"]},{"title":"数据填充","url":"/2022/11/20/数据填充/","content":"\n# 数据填充\n<!--more-->\n**示例数据**  \n本文所使用的示例数据创建如下：\n\n```python\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({\n    'name': ['Bob', 'Mary', 'Peter', np.nan, 'Lucy'],\n    'score': [99, 100, np.nan, 91, 95],\n    'class': ['class1', 'class2', 'class1', 'class2', np.nan],\n    'sex': ['male', 'fmale', 'male', 'male', 'fmale'],\n    'age': [23, 25, 20, 19, 24]\n})\n```\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/f0833b1b1dac45929ebe50fb140609aa.png)\n\n## 一、均值填充\n\n- 适用数据类型：数值类型\n- 适用场景：数据整体极值差异不大时\n- 举例：对成年男性身高的缺失值进行填充\n- 代码示例：对`data`数据中的`score`进行均值填充\n\n```python\ndata['score'].fillna(data['score'].mean())\n```\n\n```python\n# 结果如下\n0     99.00\n1    100.00\n2     96.25\n3     91.00\n4     95.00\n```\n\n## 二、中位数填充\n\n- 适用数据类型：数值类型\n- 适用场景：数据整体极值差异较大时\n- 举例：对人均收入进行填充（数据中含有高收入人群：如马总）\n- 代码示例：对`data`数据中的`score`进行中位数填充\n\n```python\ndata['score'].fillna(data['score'].median())\n```\n\n```python\n# 结果如下\n0     99.0\n1    100.0\n2     97.0\n3     91.0\n4     95.0\n```\n\n## 三、众数填充\n\n- 适用数据类型：字符类型｜没有大小关系的数值类型数据\n- 适用场景：大多数情况下\n- 举例：对城市信息的缺失进行填充/对工人车间编号进行填充\n- 代码示例：对`data`数据中的`class`进行众数填充（注意：众数填充时要通过索引0进行取值，一组数据的众数可能有多个，索引为0的数据一定会存在）\n\n```python\ndata['class'].fillna(data['class'].mode()[0])\n```\n\n```python\n# 结果如下\n0    class1\n1    class2\n2    class1\n3    class2\n4    class1\n```\n\n## 四、前后数据填充\n\n- 适用数据类型：数值类型｜字符类型\n- 适用场景：数据行与行之间具有前后关系时\n- 举例：学年成绩排行中的某同学某科目成绩丢失\n- 代码示例：对`data`数据中的`score`进行前后数据填充\n\n```python\n# 前文填充\ndata['score'].fillna(method='pad')\n# 后文填充\ndata['score'].fillna(method='bfill')\n```\n\n```python\n# 前文填充结果\n0     99.0\n1    100.0\n2    100.0\n3     91.0\n4     95.0\n# 后文填充结果\n0     99.0\n1    100.0\n2     91.0\n3     91.0\n4     95.0\n```\n\n## 五、自定义数据填充\n\n- 适用数据类型：数值类型｜字符类型\n- 适用场景：业务规定外的数据\n- 举例：某调查问卷对婚后幸福程度进行调查，到那时很多人是未婚，可以自定义内容表示未婚人群\n- 代码示例：对`data`数据中的`name`进行自定义数据填充\n\n```python\ndata['name'].fillna('no_name')\n```\n\n```python\n# 结果如下\n0        Bob\n1       Mary\n2      Peter\n3    no_name\n4       Lucy\n```\n\n## 六、Pandas插值填充\n\n- 适用数据类型：数值类型\n- 适用场景：数据列的含义较为复杂，需要更精确的填充方法时\n- 举例：对所有带有`nan`的数值列`dataframe`进行填充\n- 说明：`pandas`中进行空值填充的方法为`interpolate()`，该方法的本质是使用各种数学（统计学）中的插值方法进行填充，其中包含最近邻插值法、阶梯插值、线性插值、B样条曲线插值等多种方法。\n- 参数说明：[interpolate()参数介绍](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html)\n- 代码示例：\n\n```python\ndata['score'].interpolate()\n```\n\n```python\n# 结果如下\n0     99.0\n1    100.0\n2     95.5\n3     91.0\n4     95.0\n```\n\n## 七、机器学习算法填充\n\n- 适用数据类型：数值类型｜字符类型\n- 适用场景：具有多种数据维度的场景\n- 说明：可以选择不同的回归｜分类模型对数据进行填充\n- 注意：下面的例子中不考虑具体场景，只是用于举例\n- 数值类型数据填充代码示例（线性回归）：\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\n# 获取数据\ndata_train = data.iloc[[0, 1, 3]]\n\ndata_train_x = data_train[['age']]\ndata_train_y = data_train['score']\n# 使用线性回归进行拟合\nclf = LinearRegression()\nclf.fit(data_train_x, data_train_y)\n# 使用预测结果进行填充\ndata['score'].iloc[2] = clf.predict(pd.DataFrame(data[['age']].iloc[2]))\n```\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/b1f90b5bb09f4377923a82897f8dac21.png)\n\n- 字符类型数据填充代码示例（决策树）：\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\n\n# 获取数据\ndata_train = data.iloc[[0, 1, 3]]\n\ndata_train_x = data_train[['age']]\ndata_train_y = data_train['class']\n# 使用决策树进行拟合\nclf = DecisionTreeClassifier()\nclf.fit(data_train_x, data_train_y)\n# 使用分类结果进行填充\ndata['class'].iloc[4] = clf.predict(pd.DataFrame(data[['age']].iloc[4]))[0]\n```\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/1d621ff3531d4768beb8c618ecb7591f.png)\n","tags":["pandas数据预处理"],"categories":["deep learning"]},{"title":"交叉熵 Cross Entropy简单理解","url":"/2022/10/11/交叉熵 Cross Entropy/","content":"\n# 交叉熵 Cross Entropy\n\n在图像分类任务中，常使用交叉熵损失函数来评估模型的表现\n<!--more-->\n\n------\n\n## 1.分类错误率\n\n 就是直接计算判断失败的概率，然后对比，这种计算方法有很大的问题，他无法展现判断成功的那个是精准预测还是以非常微弱的优势判断正确，这对改进模型有较大的阻碍。\n\n## 2.均方误差\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGly1h809gwxnvlj312g052q3f.jpg\" alt=\" image-20221110193454569\" style=\"zoom:50%;\" />\n\n不难看出，MSE能够判断出来是否准确预测的状况，那为什么不采样这种损失函数呢？主要原因是在分类问题中，使用sigmoid/softmx得到概率，配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况\n\n## 3.交叉熵损失函数\n\n### (1)二分\n\n在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 p 和 1−p ，此时表达式为（log 的底数是 e）：\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGly1h809yhb0d4j30wo05yglx.jpg\" alt=\"image-20221110195149583 \" style=\"zoom:33%;\" />\n\n\\- yi —— 表示样本 i 的label，正类为 1 ，负类为 0 \n\\- pi —— 表示样本 i 预测为正类的概率\n\n### (2)多分\n\n多分类的情况实际上就是对二分类的扩展，公式如下：\n\n![image-20221110200034924](https://tva1.sinaimg.cn/large/008vxvgGly1h80a7l4jlrj31020duabb.jpg)\n\n若预测值为下图\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGly1h80a94ug8lj314c08w0tg.jpg\" alt=\"image-20221110200203807 \" style=\"zoom:33%;\" />\n\n则可得到\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGgy1h80ho0aj2wj312k0dqwg1.jpg\" alt=\"image-20221111001829637 \" style=\"zoom:50%;\" />\n\n这种可以很清晰明了的表示该模型的预测水平\n","tags":["损失函数"],"categories":["deep learning"]},{"title":"线性代数(深度学习版)","url":"/2022/10/05/线性代数(深度学习版)/","content":"\n# 线性代数(深度学习版)\n\n听李沐老师讲课做的笔记\n<!--more-->\n## 标量\n\n标量由只有一个元素的张量表示。\n\n就像\n\n```\n[3]\n```\n\n可以用一下方法生成出标量并运算\n\n```python\nimport torch\n\nx = torch.tensor(3.0)\ny = torch.tensor(2.0)\n\nx + y, x * y, x / y, x**y\n```\n\n\n\n## 向量\n\n你可以将向量视为标量值组成的列表。 我们将这些标量值称为向量的*元素*（element）或*分量*（component）。我们通过一维张量处理向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。\n\n就像\n\n```\n[0,1,2,3,4,5]\n```\n\n在pytorch中直接可以生成\n\n```python\nx = torch.arange(4)\nx\n```\n\n## 矩阵\n\n正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。 矩阵，我们通常用粗体、大写字母来表示,如下图示\n\n![image-20221016145421655](https://tva1.sinaimg.cn/large/008vxvgGgy1h774v9swt7j31400cojst.jpg)\n\n## 张量算法\n\n### 张量乘标量\n\n将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n\n### 内积\n\n两个矩阵A、B对应分量乘积之和，结果为一个标量，记作<A,B>（与向量的内积/点积/数量积的定义相似）。所以A、B的行数列数都应相同，且有结论<A,B>=tr(*A^T** B)。与哈达吗积点区别就是，一个乘完是标量，一个乘完是矩阵\n\n![image-20221102155839244](https://tva1.sinaimg.cn/large/008vxvgGgy1h7qu9ga7tkj30sm05saaa.jpg)\n\n### 矩阵乘矩阵（哈达玛积）\n\n就是按照位对位的乘\n\n### 矩阵乘矩阵（另一种）\n\n简单理解就是把A从竖着变成横着和B对位相乘\n\n### 矩阵向量积\n\n在代码中使用张量表示矩阵-向量积，我们使用与点积相同的`mv`函数。 当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。 注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。\n\n复杂理解就是\n\n![image-20221016164000897](https://tva1.sinaimg.cn/large/008vxvgGgy1h777x7vhiej311w0u0whp.jpg)\n\n## 降维\n\n通过sum来实现降维，实例如下\n\n![image-20221016153941595](https://tva1.sinaimg.cn/large/008vxvgGgy1h7766fx2kxj314q0aa74y.jpg)\n\n当然也可以规定降哪个维\n\n![image-20221016154106485](https://tva1.sinaimg.cn/large/008vxvgGgy1h7767wydbej314q0bc0tq.jpg)\n\n通过axis或者dim参数就可以调整，当调整时就遵循（层，行，列）的顺序进行更改,也就是说，降哪个维就是把哪个维归一\n\n## 非降维\n\n可以使用keepdims=True这个参数让张量在运算后的轴数不变\n\n![image-20221016161147504](https://tva1.sinaimg.cn/large/008vxvgGgy1h7773u8rtfj314o0fswfh.jpg)\n\n## 点积\n\n相同位置的张量相乘并相加，命令如下\n\n```python\ny = torch.ones(4, dtype = torch.float32)\nx, y, torch.dot(x, y)\n```\n\n## 广播\n\n### 条件\n\n当一对张量满足下面的条件时，它们才是可以被“广播”的。\n\n1、每个张量至少有一个[维度](https://so.csdn.net/so/search?q=维度&spm=1001.2101.3001.7020)。\n\n2、迭代维度尺寸时，从**尾部**（也就是从后往前）开始，依次每个维度的尺寸必须满足以下之一：\n\n- **相等**。\n\n- 其中一个张量的维度**尺寸为1**。\n\n- 其中一个张量**不存在**这个维度。\n\n  ```python\n  import torch\n  \n  # 示例1：相同形状的张量总是可广播的，因为总能满足以上规则。\n  x = torch.empty(5, 7, 3)\n  y = torch.empty(5, 7, 3)\n  \n  \n  # 示例2：不可广播（ a 不满足第一条规则）。\n  a = torch.empty((0,))\n  b = torch.empty(2, 2)\n  \n  \n  # 示例3：m 和 n 可广播：\n  m = torch.empty(5, 3, 4, 1)\n  n = torch.empty(   3, 1, 1)\n  # 倒数第一个维度：两者的尺寸均为1\n  # 倒数第二个维度：n尺寸为1\n  # 倒数第三个维度：两者尺寸相同\n  # 倒数第四个维度：n该维度不存在\n  \n  \n  # 示例4：不可广播，因为倒数第三个维度：2 != 3\n  p = torch.empty(5, 2, 4, 1)\n  q = torch.empty(   3, 1, 1)\n  ```\n\n### 用法\n\n如果张量x和张量y是可广播的，那么广播后的张量尺寸按照如下方法计算：**如果x和y的维数不等，在维数较少的张量上添加尺寸为 1 的维度。结果维度尺寸是x和y相应维度尺寸的较大者。**\n\n```python\n# 示例5：可广播\nc = torch.empty(5, 1, 4, 1)\nd = torch.empty(   3, 1, 1)\n(c + d).size()  # torch.Size([5, 3, 4, 1])\n\n\n# 示例6：可广播\nf = torch.empty(      1)\ng = torch.empty(3, 1, 7)\n(f + g).size()  # torch.Size([3, 1, 7])\n\n\n# 示例7：不可广播\no = torch.empty(5, 2, 4, 1)\nu = torch.empty(   3, 1, 1)\n(o + u).size()\n\n# 报错：\n# ---------------------------------------------------------------------------\n#\n# RuntimeError                              Traceback (most recent call last)\n#\n# <ipython-input-17-72fb34250db7> in <module>()\n#       1 o=torch.empty(5,2,4,1)\n#       2 u=torch.empty(3,1,1)\n# ----> 3 (o+u).size()\n#\n# RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n```\n\n## 范数(norm)\n\n非正式地说，一个向量的*范数*告诉我们一个向量有多大。 这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。\n\n### L2范数\n\n![image-20221017112351840](https://tva1.sinaimg.cn/large/008vxvgGgy1h784elm4d1j31480bsq4x.jpg)\n\n### L1范数\n\n绝对值之和\n\n![image-20221017112447380](https://tva1.sinaimg.cn/large/008vxvgGgy1h784fidu9kj315u08wt9b.jpg)\n\n### F范数\n\n![image-20221017112518508](https://tva1.sinaimg.cn/large/008vxvgGgy1h784g225mlj315g0dymyz.jpg)\n","tags":["线性代数"],"categories":["deep learning"]},{"title":"如何安装Pytorch","url":"/2022/10/02/如何安装Pytorch/","content":"# 如何安装Pytorch\n<!--more-->\n## 一.检测自己以前是否安装过Pytorch\n\n打开终端，切换到[pytorch](https://so.csdn.net/so/search?q=pytorch&spm=1001.2101.3001.7020)安装环境下，检测程序如下：\n\n```python\nimport torch\nprint（torch.__version__）\nprint(torch.cuda.is_available())\n```\n\n若显示版本号及True则为已安装\n\n## 二.安装Pytorch\n\n首先先安装cuda和cudnn加速包,cudnn下载需要注册账号有点麻烦，这两个的坑比较少\n\n再创建一个虚拟环境\n\n```\nconda create -n your_env_name python=x.x\n```\n\n进入虚拟环境\n\n```\nactivate your_env_name\n```\n\n尝试从官网获取安装指令https://pytorch.org，如果安装很慢，就用清华源的镜像pip安装\n\n```\npip install torch===1.3.0 torchvision===0.4.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n## 三.问题解决\n\n如果用上面的命令安装成功了，那还好，若不成功，则多半是网络问题\n\n注意在下载时不要挂vpn，否则速度也会很慢\n","tags":["Pytorch"],"categories":["deep learning"]},{"title":"Anaconda常见命令合集","url":"/2022/10/01/Anaconda常见命令合集/","content":"# Anaconda常见命令合集\n简单记录经常使用的命令\n<!--more-->\n### 1）查看安装了哪些包\n\n```text\nconda list\n```\n\n### 2)查看当前存在哪些虚拟环境\n\n```text\nconda env list \nconda info -e\n```\n\n### 3)检查更新当前conda\n\n```text\nconda update conda\n```\n\n### 3.Python创建虚拟环境\n\n```text\nconda create -n your_env_name python=x.x\n```\n\nanaconda命令创建python版本为x.x，名字为your_env_name的虚拟环境。**your_env_name文件可以在Anaconda安装目录envs文件下找到**。\n\n### 4.激活或者切换虚拟环境\n\n打开命令行，输入python --version检查当前 python 版本。\n\n```text\nLinux:  source activate your_env_nam\nWindows: activate your_env_name\n```\n\n### 5.对虚拟环境中安装额外的包\n\n```text\nconda install -n your_env_name [package]\n```\n\n### 6.关闭虚拟环境(即从当前环境退出返回使用PATH环境中的默认python版本)\n\n```text\ndeactivate env_name\n或者`activate root`切回root环境\nLinux下：source deactivate \n```\n\n### 7.删除虚拟环境\n\n```text\nconda remove -n your_env_name --all\n```\n\n### 8.删除环境钟的某个包\n\n```text\nconda remove --name $your_env_name  $package_name \n```\n\n### 9、设置国内镜像\n\n[http://Anaconda.org](https://link.zhihu.com/?target=http%3A//Anaconda.org)的服务器在国外，安装多个packages时，conda下载的速度经常很慢。清华TUNA镜像源有Anaconda仓库的镜像，将其加入conda的配置即可：\n\n\\# 添加Anaconda的TUNA镜像\n\nconda config --add channels [https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/](https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/)\n\n\\# TUNA的help中镜像地址加有引号，需要去掉\n\n\\# 设置搜索时显示通道地址\n","tags":["Anaconda"],"categories":["deep learning"]},{"title":"如何重装anaconda","url":"/2022/10/01/如何重装anaconda/","content":"# 如何重装anaconda\n<!--more-->\n## 一.卸载anaconda\n\n因为之前下载过anaconda，所以各种配置都很乱，安装其他包问题不断，我决定重新安装一个anaconda，首先不要直接卸载anaconda，先运行如下命令\n\n```\nconda install anaconda-clean\nanaconda-clean--yes //会帮你备份一个配置文件，并删除之前的配置文件\n```\n\n在之后运行软件的卸载程序Uninstall-Anaconda3.exe就行了，在这之后就可以安装新的anaconda\n\n## 二.安装anacoda\n\n**下载国内的清华园镜像，否则会很慢https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&O=D**\n\n然后正常安装，注意不要让他自动配置环境\n\n装完之后自己配环境\n\n```\nE:\\Anaconda（Python需要） \nE:\\Anaconda\\Scripts（conda自带脚本） \nE:\\Anaconda\\Library\\mingw-w64\\bin（使用C with python的时候） \nE:\\Anaconda\\Library\\bin（jupyter notebook动态库）\n```\n\n装完之后检测是否安装完成\n\n```\nconda info\n```\n\n找到.condarc文件，用记事本打开配置换源\n\n```\nchannels:\n  - defaults\nshow_channel_urls: true\ndefault_channels:\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r\n  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2\ncustom_channels:\n  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\n  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud\nenvs_dirs:\n\t...       //根据你想要安装虚拟环境的路径来配\npkgs_dirs:\n\t...\t\t\t\t//根据你想要安装package cache的路径来配置\n```\n\n","tags":["Anaconda"],"categories":["deep learning"]},{"title":"pytorch的基本数据tensor","url":"/2022/09/21/pytorch的基本数据/","content":"# pytorch的基本数据tensor\n\n<!--more-->\n\nTensor，即张量，是PyTorch中的基本操作对象，可以看做是包含单一数据类型元素的多维矩阵。从使用角度来看，Tensor与NumPy的ndarrays非常类似，相互之间也可以自由转换，只不过Tensor还支持GPU的加速。\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGgy1h80j83i5zrj314s0eddk9.jpg\" alt=\"85d160a859334f14bebdcc8c21eff18e  \" style=\"zoom:50%;\" />\n\n<img src=\"https://tva1.sinaimg.cn/large/008vxvgGgy1h80j8tbe1vj30rn0kqgn9.jpg\" alt=\"9e2b11f3791a432eaf86169f5446bcf2 \" style=\"zoom:50%;\" />\n\n## torch.FloatTensor\n\ntorch.FloatTensor用于生成数据类型为浮点型的Tensor，传递给torch.FloatTensor的参数可以是列表，也可以是一个维度值。\n\n```python\nimport torch\na = torch.FloatTensor(2,3)\nb = torch.FloatTensor([2,3,4,5])\na,b\n```\n\n得到的结果是：\n\n```python\n(tensor([[1.0561e-38, 1.0102e-38, 9.6429e-39],\n         [8.4490e-39, 9.6429e-39, 9.1837e-39]]),\n tensor([2., 3., 4., 5.]))\n```\n\n## torch.IntTensor\n\ntorch.IntTensor用于生成数据类型为整型的Tensor,传递给传递给torch.IntTensor的参数可以是列表，也可以是一个维度值。\n\n```\nimport torch\na = torch.FloatTensor(2,3)\nb = torch.FloatTensor([2,3,4,5])\na,b\n```\n\n```\nimport torch\na = torch.rand(2,3)\na \n```\n\n得到：\n\n```\ntensor([[0.5625, 0.5815, 0.8221],\n        [0.3589, 0.4180, 0.2158]])\n```\n\n## torch.randn\n\n用于生成数据类型为浮点数且维度指定的随机Tensor，和在numpy中使用的numpy.randn生成的随机数的方法类似，随机生成的浮点数的取值满足均值为0，方差为1的正态分布。\n\n```\nimport torch\na = torch.randn(2,3)\na \n```\n\n得到：\n\n```\ntensor([[-0.0067, -0.0707, -0.6682],\n        [ 0.8141,  1.1436,  0.5963]])\n```\n\n## torch.range\n\ntorch.range用于生成数据类型为浮点型且起始范围和结束范围的Tensor，所以传递给torch.range的参数有三个，分别为起始值，结束值，步长，其中步长用于指定从起始值到结束值得每步的数据间隔。\n\nimport torch\na = torch.range(1,20,2)\na\n1\n2\n3\n得到：\n\n```\ntensor([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.])\n```\n\n## torch.zeros/ones/empty\n\ntorch.zeros用于生成数据类型为浮点型且维度指定的Tensor，不过这个浮点型的Tensor中的元素值全部为0。\n\ntorch.ones生成全1的数组。\n\ntorch.empty创建一个未被初始化数值的tensor,tensor的大小是由size确定,size: 定义tensor的shape ，这里可以是一个list 也可以是一个tuple\n\n```\nimport torch\na = torch.zeros(2,3)\na\n```\n\n得到：\n\n```\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n```\n\n","tags":["Pytorch"],"categories":["deep learning"]},{"title":"如何解决跨域问题","url":"/2022/09/09/9.9/","content":"\n# 如何解决跨域问题\n\n<!--more-->\n\n我目前发现的比较简单的方法是在后端配置类中写一个Cors Config类\n\n\n```\n@Configuration\npublic class CorsConfig {\n\n    // 当前跨域请求最大有效时长。这里默认1天\n    private static final long MAX_AGE = 24 * 60 * 60;\n\n    @Bean\n    public CorsFilter corsFilter() {\n        UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource();\n        CorsConfiguration corsConfiguration = new CorsConfiguration();\n        corsConfiguration.addAllowedOrigin(\"http://localhost:8080\");// 1 设置访问源地址\n        corsConfiguration.addAllowedOrigin(\"http://localhost:8081\");//(第二个地址)\n        corsConfiguration.addAllowedHeader(\"*\"); // 2 设置访问源请求头\n        corsConfiguration.addAllowedMethod(\"*\"); // 3 设置访问源请求方法\n        corsConfiguration.setMaxAge(MAX_AGE);\n        source.registerCorsConfiguration(\"/**\", corsConfiguration); // 4 对接口配置跨域设置\n        return new CorsFilter(source);\n    }\n}\n```\n\n改成你的端口就可以解决跨域问题\n","tags":["跨域"],"categories":["后端"]},{"title":"JAVA解决端口被占用方法","url":"/2022/09/01/date9.1/","content":"# JAVA解决端口被占用方法\n在跑项目时发现明明做的一样但报错，搞了半天才发现是80端口被占用了，很生气\n\n<!--more-->\n\n简单学习了一下如何解决\n\n------\n\n第一步：win + R 输入cmd 打开运行窗口\n第二步：查看占用端口号的进程号（xxx为端口号）\n\n```\nnetstat -nao|findstr xxx\n```\n\n第三步：杀死该进程（xxx为上一步查出来的进程号）\n\n```\ntaskkill /pid xxx -f\n```\n\n有时候用idea开发时遇到项目没有关闭，软件意外关停的问题，导致重新启动项目时出现端口号被占用的情况，用本方法可以很好的解决这个问题。\n\n------\n\n最后推荐一下无意之中发现还挺好听的小调：\n\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=4233698&auto=1&height=66\"></iframe>\n","tags":["java"],"categories":["后端"]},{"title":"axios问题","url":"/2022/06/23/date9.8/","content":"\n\n\n## 在用axios时页面报错axios Unsupported protocol Http:\n<!--more-->\n原因是在0.27.2的版本下不识别url下的：\n\n所以我目前了解到的办法就是降级\n\n```undefined\nnpm install axios@0.26.1\n```\n\n之后一下成功\n\n","tags":["后端"],"categories":["后端"]},{"title":"一个小失误","url":"/2022/01/23/deploy/","content":"# 一个小失误\n\n一个小小的失误，造成了我的hexo的上传一直失败（我的内心是崩溃的），我在谷歌上到处搜解决办法，发现中文的回答几乎都一个样（这里要批评一下），最后还是在撇脚的英文翻译下终于解决了。\n\n<!--more-->\n\n**今日份的分享：**\n\n![](https://s3.bmp.ovh/imgs/2022/07/20/6973ffe95ac81fca.jpg)\n","tags":["hexo"],"categories":["杂谈"]},{"title":"个人尝试","url":"/2022/01/23/try/","content":"\n# 个人尝试\n<!--more-->\n## 为什么写博客\n\n一直很纠结要不要写博客，但是思来想去，决定开始制作并更新一个博客，就当给自己的生活增加一点内容吧。\n\n## 博客的内容\n\n目前还没有想好，可能是生活趣事？或者是像其他个人播客一样去教一些教程（本人太菜了可能教不了啥）？总之，先建立一个个人主页，剩下的之后再说吧。\n\n\n\n## tips\n\n我目前比较想在每篇文章后加一个个人的摄影分享，展示一下最近的摄影成功。\n\n**这次的摄影分享就是：**\n\n![](https://s3.bmp.ovh/imgs/2022/07/20/e42d21b4ffe366ff.jpg)\n","tags":["杂谈"],"categories":["杂谈"]}]